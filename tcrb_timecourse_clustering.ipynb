{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering TCR timecourse data\n",
    "\n",
    "This notebook will explore clustering the TCR data from the Adaptive Biotechnology time series. There are two published clustering algorithms, which appear to be highly similar.\n",
    "\n",
    "One is from Mark Davis's group at Stanford called [Gliph](https://github.com/immunoengineer/gliph). The other is from Paul Thomas's group at St Jude Childrenâ€™s Research Hospital called [tcr-dist](https://github.com/phbradley/tcr-dist).\n",
    "\n",
    "These algorithms are quite slow, and are designed for 100-1000 TCRs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Gliph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, pandas as pd, numpy as np\n",
    "import seaborn as sns, networkx as nx, os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gliph_format(adaptive_table, metadata):\n",
    "    \"\"\"\n",
    "    convert tables from Adaptive format to gliph format\n",
    "    \n",
    "    The required columns are the CDR3 aa sequence, the V and J allele,\n",
    "    And the patient identifier (or sample).\n",
    "    \n",
    "    Let's first try the column \"PatientCounts\" as the individual\n",
    "    \"\"\"\n",
    "    gliph_table = adaptive_table[['aminoAcid', 'v', 'j']]\n",
    "    gliph_table['PatientCounts'] = adaptive_table['sample'].apply(lambda x: metadata.set_index('sample_short').to_dict()['individual'][x])\n",
    "    \n",
    "    return gliph_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timecourse_aa = pd.read_csv('all_subjects_timecourse_w_nuc_aa.tsv.gz', \n",
    "                            sep='\\t', compression='gzip')\n",
    "metadata = pd.read_csv('tcrb_timecourse_metadata.txt', sep='\\t')\n",
    "epi_occ = pd.read_csv('tcr_epitopes_occurence_all.tsv.gz',\n",
    "                                sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's first look at persistant TCRs from subject01 from sample s1_110316_PBMC\n",
    "timecourse_aa_sub1 = timecourse_aa.loc[timecourse_aa['sample'] == 's1_110316_PBMC']\n",
    "\n",
    "pers_tcrs_sub1 = epi_occ.loc[epi_occ['num_occ_in_sub1_pbmc'] == 8]['tcr_epitope'].values\n",
    "\n",
    "#now get timecourse with only persistent tcrs\n",
    "timecourse_pers_sub1 = timecourse_aa_sub1.loc[timecourse_aa_sub1['tcr_epitope'].isin(pers_tcrs_sub1)]\n",
    "\n",
    "#get final table\n",
    "timecourse_pers_sub1_gliph = gliph_format(timecourse_pers_sub1, metadata)\n",
    "\n",
    "#write to file\n",
    "timecourse_pers_sub1_gliph.to_csv('s1_111014_PBMC_pers_gliphtest.tsv',\n",
    "                                 sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after running the gliph pipeline, let's examine the results\n",
    "clone_ntwk = pd.read_csv('s1_111014_PBMC_pers_gliphtest.tsv-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "\n",
    "conv_groups = pd.read_csv('subject01_randomtcrs_7012-convergence-groups.txt',\n",
    "                         sep='\\t', header=None, names=['group_size',\n",
    "                                                      'group_name',\n",
    "                                                      'group_members'])\n",
    "\n",
    "motifs = pd.read_csv('s1_111014_PBMC_pers_gliphtest.tsv-kmer_resample_1000_minp0.001_ove10.txt',\n",
    "                    sep='\\t')\n",
    "\n",
    "kmer_resample = pd.read_csv('s1_111014_PBMC_pers_gliphtest.tsv-kmer_resample_1000_log.txt',\n",
    "                           sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file clone network is 3 columns, with each row representing a pair of cdr3s that were significantly clustered together. The first two are lists of cdr3 regions that are \"linked\" or clustered together. The final columns is the type of convergence that resulted in the link: global, local, or singleton. Singletons are clones that have no links. Local are links based of off amino acid motifs or kmers. Global are links based on global similarity.\n",
    "\n",
    "The congergence-groups file is the same information but in a different format. This file also has 3 columns, with each row representing a cluster of cdr3s. The first column is the number of cdr3s in that cluster. The second column is the name of the cluster. The third column is the membership of that cluster, separated by spaces.\n",
    "\n",
    "The ove10 file provides amino acid motifs or kmers that were deemed significantly enriched in the provided tcr set. 'Motif' is the amino acid motif. 'Counts' is the number of occurences in the provided TCR set. 'avgRef' is the average number of occurences in the subsampled reference sets. 'topRef' is the maximum number of occurences in the subsamples reference sets. OvE is something close (but not exactly) the fold-enrichment of the provided TCR set versus the resampled reference sets. p-value is the p-value for this motif.\n",
    "\n",
    "The kmer_resample file is the results from the N kmer resamples of the reference data set. the \"Discovery\" row is the results from the provided TCR set, and then each subsequent row is the results from a simulated resampling.\n",
    "\n",
    "Looking at the data, 5.5% of the TCRs clustered with at least one other TCR.\n",
    "\n",
    "Clustering questions:\n",
    "\n",
    "1. Do persitent TCRs cluster to a greater or lesser extent than other TCRs?\n",
    "2. Do public TCRs cluster to a greater or lesser extent?\n",
    "3. Do both public and persistent TCRs cluster more or less across individuals?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first test to see how greatly the --global_vgene=0 flag changes results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the --global_vgene=1 flag doesn't greatly change the results. The difference of using the flag is about the same as doing the analysis again with no flag (i.e. the difference is within the sampling noise). So let's roll with the -vgene flag because it's suggested and why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check on a single table with persistent TCRs from all individuals\n",
    "sub1_pers = epi_occ.loc[epi_occ['num_occ_in_sub1_pbmc'] == 8]['tcr_epitope']\n",
    "sub2_pers = epi_occ.loc[epi_occ['num_occ_in_sub2_pbmc'] == 8]['tcr_epitope']\n",
    "sub3_pers = epi_occ.loc[epi_occ['num_occ_in_sub3_pbmc'] == 8]['tcr_epitope']\n",
    "\n",
    "#Get only one time point of data, arbitrarily choose the first\n",
    "pers_sub1_df = timecourse_aa.loc[timecourse_aa['sample'] =='s1_110316_PBMC']\n",
    "pers_sub2_df = timecourse_aa.loc[timecourse_aa['sample'] =='s2_110317_PBMC']\n",
    "pers_sub3_df = timecourse_aa.loc[timecourse_aa['sample'] =='s3_110316_PBMC']\n",
    "\n",
    "#filter for persistent TCRs\n",
    "pers_sub1_df = pers_sub1_df.loc[pers_sub1_df['tcr_epitope'].isin(sub1_pers)]\n",
    "pers_sub2_df = pers_sub2_df.loc[pers_sub2_df['tcr_epitope'].isin(sub2_pers)]\n",
    "pers_sub3_df = pers_sub3_df.loc[pers_sub3_df['tcr_epitope'].isin(sub3_pers)]\n",
    "\n",
    "#add individual\n",
    "pers_sub1_df['PatientCounts'] = 'subject01'\n",
    "pers_sub2_df['PatientCounts'] = 'subject02'\n",
    "pers_sub3_df['PatientCounts'] = 'subject03'\n",
    "\n",
    "#combine and write to file\n",
    "pers_df = pd.concat([pers_sub1_df, pers_sub2_df, pers_sub3_df])\n",
    "\n",
    "pers_gliph = pers_df[['aminoAcid', 'v', 'j', 'PatientCounts']]\n",
    "pers_gliph = pers_gliph.rename(columns={'aminoAcid': 'CDR3b', 'v': 'TRBV',\n",
    "                                       'j': 'TRBJ'}) #rename columns\n",
    "pers_gliph.to_csv('gliph/pers_tcrs_PBMC_allsub.txt', sep='\\t',\n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do a preliminary test of random sets of 7012 TCRs (# of persistent TCRs in subject01)\n",
    "#have to consider from what TCRs to pick and whether to weight the TCRs.\n",
    "#without weighting, one risks picking too many TCRs that might be errors\n",
    "\n",
    "#first get only tcrs from subject01\n",
    "epi_occ_sub1 = epi_occ.loc[epi_occ['num_occ_in_sub1_pbmc'] > 0]\n",
    "\n",
    "#pick them randomly from all TCRs, pick 100 sets\n",
    "tcrs_rand1 = [np.random.choice(epi_occ_sub1['tcr_epitope'].values, \n",
    "                               size=7012) for x in range(100)]\n",
    "\n",
    "#pick them randomly from nonpersistent TCRs\n",
    "epi_occ1_nonpers = epi_occ_sub1.loc[(epi_occ_sub1['num_occ_in_sub1_pbmc'] != 8)]\n",
    "tcrs_rand_nonpers = [np.random.choice(epi_occ1_nonpers['tcr_epitope'].values, \n",
    "                                      size=7012) for x in range(100)]\n",
    "\n",
    "#pick them randomly from each category of occurence\n",
    "tcr_sets = []\n",
    "for x in [1, 2, 3, 4, 5, 6, 7]:\n",
    "    tmp = epi_occ_sub1.loc[epi_occ['num_occ_in_sub1_pbmc'] == x]\n",
    "    tcr_sets.append([np.random.choice(tmp['tcr_epitope'].values, \n",
    "                                      size=7012) for x in range(100)])\n",
    "    \n",
    "#write these outputs to a table\n",
    "timecourse_aa.loc[timecourse_aa['sample'] =='s1_110316_PBMC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subset_table_to_gliph(df, tcrs, patient_counts='unknown', output_file='gliph.txt'):\n",
    "    \"\"\"\n",
    "    With a df, subset the df to only include the TCRs in the list tcrs.\n",
    "    \n",
    "    Then output a gliph table to a file\n",
    "    \"\"\"\n",
    "    df = df.loc[df['tcr_epitope'].isin(tcrs)]\n",
    "    df = df[['aminoAcid', 'v', 'j']]\n",
    "    df = df.drop_duplicates()\n",
    "    df['PatientCounts'] = patient_counts\n",
    "    df = df.rename(columns={'aminoAcid': 'CDR3b', 'v': 'TRBV', 'j': 'TRBJ'})\n",
    "    df.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_groups_pers = pd.read_csv('s1_111014_PBMC_pers_JL272017_vgene/s1_111014_PBMC_pers.tsv-convergence-groups.txt',\n",
    "                         sep='\\t', header=None, names=['group_size',\n",
    "                                                      'group_name',\n",
    "                                                      'group_members'])\n",
    "pers_tcrs = Counter(conv_groups_pers['group_size'])\n",
    "clone_ntwk_pers = pd.read_csv('s1_111014_PBMC_pers_JL272017_vgene/s1_111014_PBMC_pers.tsv-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "pers_edges = Counter(clone_ntwk_pers['convergence_type'])\n",
    "\n",
    "conv_groups1 = pd.read_csv('subject01_occ1_tcrs_7012-convergence-groups.txt',\n",
    "                         sep='\\t', header=None, names=['group_size',\n",
    "                                                      'group_name',\n",
    "                                                      'group_members'])\n",
    "occ1_tcrs = Counter(conv_groups1['group_size'])\n",
    "clone_ntwk_occ1 = pd.read_csv('subject01_occ1_tcrs_7012-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "occ1_edges = Counter(clone_ntwk_occ1['convergence_type'])\n",
    "\n",
    "conv_groups2 = pd.read_csv('subject01_occ2_tcrs_7012-convergence-groups.txt',\n",
    "                         sep='\\t', header=None, names=['group_size',\n",
    "                                                      'group_name',\n",
    "                                                      'group_members'])\n",
    "occ2_tcrs = Counter(conv_groups2['group_size'])\n",
    "clone_ntwk_occ2 = pd.read_csv('subject01_occ2_tcrs_7012-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "occ2_edges = Counter(clone_ntwk_occ2['convergence_type'])\n",
    "\n",
    "clone_ntwk_occ3 = pd.read_csv('subject01_occ3_tcrs_7012-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "occ3_edges = Counter(clone_ntwk_occ3['convergence_type'])\n",
    "\n",
    "clone_ntwk_occ4 = pd.read_csv('subject01_occ4_tcrs_7012-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "occ4_edges = Counter(clone_ntwk_occ4['convergence_type'])\n",
    "\n",
    "clone_ntwk_occ5 = pd.read_csv('subject01_occ5_tcrs_7012-clone-network.txt',\n",
    "                        sep='\\t', header=None, names=['cdr3_1', \n",
    "                                                      'cdr3_2', \n",
    "                                                      'convergence_type'])\n",
    "occ5_edges = Counter(clone_ntwk_occ5['convergence_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at these broadly, my hunch is that clustering is signifcantly less for TCRs that only occur once or potentially Naive T cells, and that memory T cells or T cells that occur more than once have a greater tendency to cluster. If you look at the % TCRs that are singletons, the differece is slight (96-97% versus 91-94%), but there might be better ways to look at it. For example, the number of global convergences is 5.5X in persistent TCRs versus TCRs only in a single sample.\n",
    "\n",
    "ALSO be careful when picking random sets of TCRs. There are more persistent TCRs than TCRs in 5, 6, or 7 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "with open('s1_111014_PBMC_pers_JL272017_vgene/s1_111014_PBMC_pers.tsv-clone-network.txt', \n",
    "          'r+') as network_file:\n",
    "    for line in network_file:\n",
    "        line_split = line.rstrip().split('\\t')\n",
    "        if line_split != 'singleton':\n",
    "            g.add_edge(line_split[0], line_split[1])\n",
    "\n",
    "nx.average_clustering(g)\n",
    "\n",
    "h = nx.Graph()\n",
    "with open('subject01_occ1_tcrs_7012-clone-network.txt', \n",
    "          'r+') as network_file:\n",
    "    for line in network_file:\n",
    "        line_split = line.rstrip().split('\\t')\n",
    "        if line_split != 'singleton':\n",
    "            h.add_edge(line_split[0], line_split[1])\n",
    "\n",
    "nx.average_clustering(h)\n",
    "\n",
    "i = nx.Graph()\n",
    "with open('subject01_occ2_tcrs_7012-clone-network.txt', \n",
    "          'r+') as network_file:\n",
    "    for line in network_file:\n",
    "        line_split = line.rstrip().split('\\t')\n",
    "        if line_split != 'singleton':\n",
    "            i.add_edge(line_split[0], line_split[1])\n",
    "\n",
    "nx.average_clustering(i)\n",
    "\n",
    "j = nx.Graph()\n",
    "with open('subject01_occ3_tcrs_7012-clone-network.txt', \n",
    "          'r+') as network_file:\n",
    "    for line in network_file:\n",
    "        line_split = line.rstrip().split('\\t')\n",
    "        if line_split != 'singleton':\n",
    "            j.add_edge(line_split[0], line_split[1])\n",
    "\n",
    "nx.average_clustering(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
